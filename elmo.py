# -*- coding: utf-8 -*-
"""ELMo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F2N0uFL3f52blXhBNkMGdBpWiz_1gQ9q
"""

#Step 1:Importing all the packages whichever is do you want to include.

#from google.colab import drive
#drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import spacy
#from tqdm import tqdm
import re
#import time
#import pickle
pd.set_option('display.max_colwidth',200)

#Step 2:Reading the data and Inspecting.

train=pd.read_csv('train_2kmZucJ.csv')
test=pd.read_csv('test_oJQbWVk.csv')



train['label'].value_counts(normalize=True)


#Step 3:Text cleaning and Preprocessing
#Removing url from the train dataset.

train['cleen_tweet']=train['tweet'].apply(lambda x:re.sub(r'http\S+', '',x))
test['cleen_tweet']=test['tweet'].apply(lambda x:re.sub(r'http\S+', '',x))



#So we are removing more non-related data from datasets
punctuation = '!"#$%&()*+-/:;<=>?@[\\]^_`{|}~'

train['cleen_tweet']=train['cleen_tweet'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))
test['cleen_tweet']=test['cleen_tweet'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))



#converting into lower case
train['cleen_tweet']=train['cleen_tweet'].str.lower()
test['cleen_tweet']=test['cleen_tweet'].str.lower()


#remove numbers fron cleen_tweet
train['cleen_tweet']=train['cleen_tweet'].str.replace('[0-9]',' ')
test['cleen_tweet']=test['cleen_tweet'].str.replace('[0-9]',' ')

#removing white spaces
train['cleen_tweet']=train['cleen_tweet'].apply(lambda x:' '.join(x.split()))
test['cleen_tweet']=test['cleen_tweet'].apply(lambda x:' '.join(x.split()))



#Identifinding various from of the same word.
nlp=spacy.load('en',disable=['parser','ner'])


def lemmatization(texts):
    output=[]
    for i in texts:
      s=[token.lemma_ for token in nlp(i)]
      output.append(' '.join(s))
    return output

train['cleen_tweet']=lemmatization(train['cleen_tweet'])
test['cleen_tweet']=lemmatization(test['cleen_tweet'])

train['cleen_tweet']=lemmatization(train['cleen_tweet'])
test['cleen_tweet']=lemmatization(test['cleen_tweet'])

f=pd.read_csv('bad_words.csv')

for i in range(len(f)):
  s=f.loc[i].at['bad_words']
  for j in range(len(train)):
    c=train.loc[j].at['cleen_tweet'].split(' ')
   
    for k in range(len(c)):
      b=str(''.join(c[k]))
      if(s==b):
        train['cleen_tweet']=train['cleen_tweet'].str.replace(str(b),'$&@*#')

for i in range(len(f)):
  q=f.loc[i].at['bad_words']
  for j in range(len(test)):
    w=test.loc[j].at['cleen_tweet'].split(' ')
   
    for k in range(len(w)):
      e=str(''.join(w[k]))
      if(q==e):
        test['cleen_tweet']=test['cleen_tweet'].str.replace(str(e),'$&@*#')

sub = pd.DataFrame({'id':train['id'], 'label':train['cleen_tweet']})

# write predictions to a CSV file
sub.to_csv("Solution.csv", index=False)



